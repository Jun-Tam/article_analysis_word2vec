{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) Application to Articles\n",
    "\n",
    "This NLP project demonstrates NLP application to energy industry articles in PDF files. <br>\n",
    "\n",
    "### Part1: Preprocessing\n",
    "\n",
    "Preprocessing part is described: conversion from PDF to text, tokenizer, duplicate file deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "import hashlib\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "from calendar import month_name\n",
    "import PyPDF2\n",
    "from english_spelling import replace_gb2us\n",
    "import nltk\n",
    "_ = nltk.download('wordnet', quiet=True)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text_list):\n",
    "    '''\n",
    "    Convert a list of strings into a single body text.\n",
    "    Exclude some words and Client Help Desk Info\n",
    "    '''\n",
    "    \n",
    "    # In case text.split('\\n') does not work, create a list of words\n",
    "    if len(max(text_list, key=len)) > 100 and len(text_list) < 35:\n",
    "        text_list = ' '.join(text_list).split()\n",
    "    \n",
    "    # Useless word list\n",
    "    exclude = ['Page', 'Insight - ','Executive summary', 'Key take-aways:','Summary','INSIGHT']\n",
    "    filter_func = lambda s: not any(x in s for x in exclude) and len(s) > 1\n",
    "    text_list = [line.replace(\"'\",'') for line in text_list if filter_func(line)]\n",
    "    \n",
    "    # Remove Help Desk Information\n",
    "    flag = 0    \n",
    "    try:\n",
    "        flag = 1\n",
    "        idx_ch = text_list.index('Client Helpdesk')\n",
    "        text_list = text_list[:idx_ch]\n",
    "    except:\n",
    "        pass;\n",
    "\n",
    "    body_text = ' '.join(text_list)\n",
    "    return body_text,flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_some_chars(body_text):\n",
    "    ''' Delete symbols '''\n",
    "    char_exclude = [',', '.','--','-','+','!','?',':',';','\"','(',')',']','[',\n",
    "                    '@','^','*','>','<','`','%','$','/']\n",
    "    body_text = ''.join([s for s in body_text if s not in set(char_exclude)])\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_non_ascii(body_text):\n",
    "    ''' Delete non-ascii characters '''\n",
    "    printable = set(string.printable)\n",
    "    body_text = ''.join(filter(lambda x: x in printable, body_text))\n",
    "    body_text = re.sub(' +', ' ', body_text)\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appleOrange(body_text):\n",
    "    ''' Split 'appleOrange' to 'apple' and 'Orange' '''\n",
    "    word_list = body_text.split(' ')\n",
    "    for i, line in enumerate(word_list):        \n",
    "        try:\n",
    "            r1 = re.findall('([A-Z][a-z]+)',line)[-1]\n",
    "            r2 = line.replace(r1, '')\n",
    "            word_list[i] = r2 + ' ' + r1\n",
    "        except:\n",
    "            pass;\n",
    "    body_text = ' '.join(word_list)[1:]\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_finder(body_text):\n",
    "    ''' Extract Date from body text '''\n",
    "    \n",
    "    s = body_text[0:100]\n",
    "    \n",
    "    # Extract Month\n",
    "    pattern = '|'.join(month_name[1:])\n",
    "    month = re.search(pattern, s, re.IGNORECASE).group(0)\n",
    "    \n",
    "    # Extract Year\n",
    "    year = re.search(r'\\d{4}', s).group()\n",
    "    date_str = ' '.join([month,year])\n",
    "    date = datetime.datetime.strptime(date_str,'%B %Y')\n",
    "    ymd_str = datetime.datetime.strftime(date,'%Y/%m/%d')\n",
    "    \n",
    "    body_text = body_text.replace(month + ' ' + year + ' ', '')\n",
    "    return body_text, ymd_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(body_text):\n",
    "    ''' Convert verbs to at its present tense, plural nouns to singular '''\n",
    "    word_list = body_text.split(' ')\n",
    "    word_list = [WordNetLemmatizer().lemmatize(word,'v') for word in word_list]   # Convert verbs to present tense\n",
    "    word_list = [WordNetLemmatizer().lemmatize(word,'n') for word in word_list]   # Convert plural to singular\n",
    "    word_list = [WordNetLemmatizer().lemmatize(word,'a') for word in word_list]   # Adjective\n",
    "    word_list = [WordNetLemmatizer().lemmatize(word,'r') for word in word_list]   # Adverb    \n",
    "    word_list = [word for word in word_list if not re.search(r'\\d',word)]         # Remove words that contain a number\n",
    "    word_list = [word for word in word_list if len(word) < 10 and len(word) > 1]  # Remove too short/long words\n",
    "    body_text = ' '.join(word_list)\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_parser(path_file):\n",
    "    ''' Parser '''\n",
    "    pdfReader = PyPDF2.PdfFileReader(open(path_file, 'rb'))  # Read a PDF file\n",
    "\n",
    "    text = ''\n",
    "    for i in range(pdfReader.numPages):\n",
    "        # Extract text from a page object        \n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        text_tmp = pageObj.extractText()\n",
    "\n",
    "        # Append text on every page\n",
    "        if i == 0:\n",
    "            text += text_tmp\n",
    "        else:\n",
    "            text += '\\n'.join(text_tmp.split('\\n')[1:])  # Append text on every page\n",
    "            \n",
    "        text_list = text.split('\\n')\n",
    "        body_text, flag = preprocessing(text_list)   # Preprocessing from text list\n",
    "        body_text = del_some_chars(body_text)        # Delete some characters\n",
    "        body_text, ymd_str = date_finder(body_text)  # Extract Date\n",
    "        body_text = del_non_ascii(body_text)         # Delete non-ascii characters\n",
    "        body_text = appleOrange(body_text)           # Split overlapping word into single words\n",
    "        body_text = body_text.lower()                # Convert all the text into lower case\n",
    "        body_text = replace_gb2us(body_text)         # Replace British English with American English \n",
    "        body_text = tokenizer(body_text)             # Tokenizer\n",
    "    return body_text, ymd_str, flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF files, pre-process text files, and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdfs(path_input,path_output):\n",
    "    ''' Load PDF files and pre-process texts and save them in ascii files '''\n",
    "    article_list = []\n",
    "    for root, dirs, files in os.walk(path_input):\n",
    "        Group = root.split('\\\\')[-2]\n",
    "        SubGroup = root.split('\\\\')[-1]\n",
    "        idx = 0\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                # Extract body text and Issue Date\n",
    "                path_file = os.path.join(root,file)\n",
    "                try:\n",
    "                    body_text, ymd_str, flag = pdf_parser(path_file)\n",
    "                except:\n",
    "                    body_text, ymd_str, flag = '', '0000/00/00', 0\n",
    "                    pass;\n",
    "\n",
    "                # Record Article Information and append it in a dictionary            \n",
    "                Title = path_file.split('\\\\')[-1].replace('.pdf','')\n",
    "                dict_add = OrderedDict({'Group':Group,'SubGroup':SubGroup,'Date':ymd_str,'Title':Title,\n",
    "                                        'Length Body Text':len(body_text)})\n",
    "                article_list.append(dict_add)\n",
    "\n",
    "                idx += 1\n",
    "                filename = Group+'_'+SubGroup+'_'+str(idx)+'_'+ymd_str.replace('/','.')+'.txt'\n",
    "                path_output_file = os.path.join(path_output, filename)\n",
    "                with open(path_output_file,'w') as file:\n",
    "                    file.write(body_text)\n",
    "    # Save Preprocessing Results\n",
    "    df = pd.DataFrame(article_list)\n",
    "    df.to_csv('./article_summary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = '.\\\\Reports_PDF'\n",
    "path_output = './articles_text'\n",
    "preprocess_pdfs(path_input,path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicated files\n",
    "Reference: https://www.pythoncentral.io/finding-duplicate-files-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDup(parentFolder):\n",
    "    # Dups in format {hash:[names]}\n",
    "    dups = {}\n",
    "    for dirName, subdirs, fileList in os.walk(parentFolder):\n",
    "        for filename in fileList:\n",
    "            # Get the path to the file\n",
    "            path = os.path.join(dirName, filename)\n",
    "            # Calculate hash\n",
    "            file_hash = hashfile(path)\n",
    "            # Add or append the file path\n",
    "            if file_hash in dups:\n",
    "                dups[file_hash].append(path)\n",
    "            else:\n",
    "                dups[file_hash] = [path]\n",
    "    return dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinDicts(dict1, dict2):\n",
    "    for key in dict2.keys():\n",
    "        if key in dict1:\n",
    "            dict1[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfile(path, blocksize = 65536):\n",
    "    afile = open(path, 'rb')\n",
    "    hasher = hashlib.md5()\n",
    "    buf = afile.read(blocksize)\n",
    "    while len(buf) > 0:\n",
    "        hasher.update(buf)\n",
    "        buf = afile.read(blocksize)\n",
    "    afile.close()\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteDups(dict1):\n",
    "    results = list(filter(lambda x: len(x) > 1, dict1.values()))\n",
    "    if len(results) > 0:\n",
    "        for result in results:\n",
    "            for subresult in result[1:]: # Leave the firstfile as is\n",
    "                os.remove(subresult)\n",
    "    else:\n",
    "        print('No duplicate files found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = {}\n",
    "folder = './articles_text/'\n",
    "if os.path.exists(folder):\n",
    "    # Find the duplicated files and append them to the dups\n",
    "    joinDicts(dups, findDup(folder))\n",
    "else:\n",
    "    print('%s is not a valid path, please verify' % folder)\n",
    "    sys.exit()\n",
    "\n",
    "deleteDups(dups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
